{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
count1s = 0
count2s = 0
i =0
for (i in 1:length(g4)){
if(g4[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
count1s = 0
count2s = 0
i =0
for (i in 1:length(g5)){
if(g5[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
plot(clustAll)
plot(clustAll,xlab="Heart Data: Binary and Continuous" )
height_mean=mean(clustAll$height)
height_sd = sd(clustAll$height)
cut_off=height_mean+(3*height_sd)
dend=plot(clustAll,hang=-1)
abline(h=cut_off, lty=2, col=2)
abline(h=cut_off+7, lty=2, col=1)
rect.hclust(clustAll,k=5,border=2:5)
clust_label = cutree(clustAll, k = 5)
classDivision = sapply(unique(clust_label), function(g)heartData$Class[clust_label ==g])
classDivision
dim(g1)
length(g1)
aggregate(standard_heart,list(group_4),median)
aggregate(heartDataCont,list(group_4),median)
round(aggregate(heartData,list(group_5),mean),2)
round(apply(heartData, 2, mean), 2)
summary(heartData)#.mean
length(g2)
round(heartData,list(group_5),2)
round(aggregate(heartData,list(group_5)),2)
?aggregate
round(display(heartData,list(group_5)),2)
length(g3)
length(g4)
length(g5)
################################################################################################################################################
#MLA Assignment
################################################################################################################################################
#Read csv file
heart_data<-read.csv("C:/Users/carty/Documents/Third Year MSISS/MLA Assignment/heart_data.csv")
heart_data
summary(heart_data)
dim(heart_data)
library(ellipse)
library(MASS)
library(dplyr)
class1data<-heart_data %>% filter(Class==1)
class2data<-heart_data %>% filter(Class==2)
dim(class1data)
dim(class2data)
plot(heart_data[,c(1,3,4,6)], col = as.factor(heart_data[,10]))
#Before we use LDA, we need to split the data into training, and test sets. (Why don't we need a validation set?)
#For this example we will try an 80:20 split.
strain <- heart_data[c(1:150), ]
stest <- heart_data[c(151:270),]
train <- rbind(class1data[1:75,],class2data[1:60,])
test <- rbind(class1data[76:150,],class2data[61:120,])
#We can then train our classifier:
lsol <- lda(train[, c(1,3,4,6,8,9)], grouping = train[,10])
#If you enter the following, you will be returned with a list of summary information concerning the computation:
lsol$prior
lsol$means
#To estimate the covariance matrices for both subsets of salmon data, enter the following:
n_class_1 <- length(class1data)
n_class_2 <- length(class2data)
single_cov_num <- ((n_class_1 - 1) * cov (class1data) + (n_class_2 - 1) * cov(class2data) )
single_cov <- single_cov_num / ( length(train[, 10]) - 2)
single_cov
#As well as providing information about prior probalilities and group means, calling lsol directly provides
#information regarding the coefficients of linear discriminants (use lsol$scaling to call this directly):
lsol
lsol$scaling
#To automatically predict the test data set enter:
predict(lsol, test[, c(1,3,4,6)])
#To automatically predict the test data set enter:
predict(lsol, test[, c(1,3,4,6,8,9)])
#Cross-Validation
#an alternative technique for measuring the performance of the model is to perform cross-validation
lsol_cv <- lda(heart_data[,c(1,3,4,6)], grouping = heart_data[, 10], CV = TRUE)
lsol_cv
plot(heart_data[, c(1,3,4,6)], col = as.factor(heart_data[, 10]), pch = as.numeric(lsol_cv$class))
#Quadratic Discriminant Analysis
#difference between QDA and LDA is that the former permits each group distribution to have its own covariance
#matrix, whilst the latter assumes a common covariance matrix for all group distributions
train2 <- rbind(class1data[1:75,],class2data[1:60,])
train2<-scale(train2[,c(1,3,4,6)])
test2 <- rbind(class1data[76:150,],class2data[61:120,])
test2<-scale(test2[,c(1,3,4,6)])
qsol <- qda(train2, grouping = train[, 10])
predict(qsol, test2)
#Again you will notice in an 80:20 training:testing split we have achieved 100% correct classification
scaleData<-scale(heart_data[,c(1,3,4,6)])
qsol_cv <- qda(scaleData, grouping = heart_data[, 10], CV = TRUE)
plot(heart_data[, c(1,3,4,6)], col = as.factor(heart_data[, 10]), pch = as.numeric(qsol_cv$class))
plot(heart_data[, c(1,3)], col = as.factor(heart_data[, 10]), pch = as.numeric(qsol_cv$class))
plot(heart_data[, c(1,4)], col = as.factor(heart_data[, 10]), pch = as.numeric(qsol_cv$class))
plot(heart_data[, c(1,6)], col = as.factor(heart_data[, 10]), pch = as.numeric(qsol_cv$class))
#To find the covariances for the two groups enter the following:
cov (alaska_salmon)
cov (canada_salmon)
summary(allVars)
################################################################################################################################################
#MLA Assignment
################################################################################################################################################
#Read csv file
heart_data<-read.csv("C:/Users/carty/Documents/Third Year MSISS/MLA Assignment/heart_data.csv")
heart_data
summary(heart_data)
dim(heart_data)
library(ellipse)
library(MASS)
library(dplyr)
WSS <- rep(0,30)
KMD<-heart_data[, c(1,3,4,6,8)]
for(i in 1:30)
{
WSS[i] <- sum(kmeans(KMD, centers = i)$withinss)
}
plot(WSS)
k <- 3
cl2 <- kmeans(KMD,centers = 3)
cl2
table(cl2$cluster)
cl2$centers
cl2$withinss
#Plot the Clusters and their centroids
plot(KMD, col = cl2$cluster)
points(cl2$centers, col=1:k, pch=8, cex=5)
#Distance between cluster centroids
dist(cl2$centers)
#To calculate the 'Average Distance from Cluster Centroid 1' enter:
g1 <- faithful[which(cl2$cluster==1),]
ng1 <- cl2$size[1]
total1 <- sum(as.matrix(dist(rbind(g1, cl2$centers[1,])))[ng1+1,])
ave1 <- total1/ng1
ave1
#To calculate the 'Average Distance from Cluster Centroid 2' enter:
g2 <- faithful[which(cl2$cluster==2),]
ng2 <- cl2$size[2]
total2 <- sum(as.matrix(dist(rbind(g2, cl2$centers[2,])))[ng2+1,])
ave2 <- total2/ng2
ave2
WSS <- rep(0,30)
KMD<-scale(heart_data[, c(1,3,4,6,8)])
#KMD<-scale(KMD)
KMD
for(i in 1:30)
{
WSS[i] <- sum(kmeans(KMD, centers = i)$withinss)
}
plot(WSS)
k <- 3
cl2 <- kmeans(KMD,centers = 3)
cl2
cl2$cluster
table(cl2$cluster)
cl2$centers
cl2$withinss
#Plot the Clusters and their centroids
plot(KMD, col = cl2$cluster)
points(cl2$centers, col=1:k, pch=8, cex=5)
#Distance between cluster centroids
dist(cl2$centers)
#To calculate the 'Average Distance from Cluster Centroid 1' enter:
g1 <- faithful[which(cl2$cluster==1),]
ng1 <- cl2$size[1]
total1 <- sum(as.matrix(dist(rbind(g1, cl2$centers[1,])))[ng1+1,])
ave1 <- total1/ng1
ave1
#To calculate the 'Average Distance from Cluster Centroid 2' enter:
g2 <- faithful[which(cl2$cluster==2),]
ng2 <- cl2$size[2]
total2 <- sum(as.matrix(dist(rbind(g2, cl2$centers[2,])))[ng2+1,])
ave2 <- total2/ng2
ave2
WSS <- rep(0,30)
KMD<-scale(heart_data[, c(1,3,4,6,8,9)])
KMD<-cbind(KMD,heart_data[,c(2,5,7)])
KMD
for(i in 1:30)
{
WSS[i] <- sum(kmeans(KMD, centers = i)$withinss)
}
plot(WSS)
k <- 3
cl2 <- kmeans(KMD,centers = 3)
cl2
cl2$cluster
table(cl2$cluster)
cl2$centers
cl2$withinss
cl2 <- kmeans(KMD,centers = 3)
cl2
group_5 = cutree(cl2,5)
print(cl2.res)
print(cl2)
round(apply(KMD, 2, mean), 2)
clust_label
#KMD<-scale(heart_data[, c(1,3,4,6,8,9)])
#KMD<-cbind(KMD,heart_data[,c(2,5,7)])
KMD<-heart_data[,1:9]
KMD
cl2
cl2$cluster
classDivision = sapply(unique(cl2$cluster), function(g)heart_data$Class[cl2$cluster ==g])
classDivision
WSS <- rep(0,30)
#KMD<-scale(heart_data[, c(1,3,4,6,8,9)])
#KMD<-cbind(KMD,heart_data[,c(2,5,7)])
KMD<-heart_data[,1:9]
KMD
for(i in 1:30)
{
WSS[i] <- sum(kmeans(KMD, centers = i)$withinss)
}
plot(WSS)
k <- 7
cl2 <- kmeans(KMD,centers = 7)
cl2
classDivision = sapply(unique(cl2$cluster), function(g)heart_data$Class[cl2$cluster ==g])
classDivision
print(cl2)
round(apply(KMD, 2, mean), 2)
cl2$cluster
table(cl2$cluster)
cl2$centers
cl2$withinss
group_5 = cutree(cl2,5)
print(cl2)
round(apply(KMD, 2, mean), 2)
#Plot the Clusters and their centroids
plot(KMD, col = cl2$cluster)
points(cl2$centers, col=1:k, pch=8, cex=5)
plot(WSS)
WSS <- rep(0,30)
#KMD<-scale(heart_data[, c(1,3,4,6,8,9)])
#KMD<-cbind(KMD,heart_data[,c(2,5,7)])
KMD<-heart_data[,1:9]
KMD
for(i in 1:30)
{
WSS[i] <- sum(kmeans(KMD, centers = i)$withinss)
}
plot(WSS)
k <- 5
cl2 <- kmeans(KMD,centers = 5)
cl2
classDivision = sapply(unique(cl2$cluster), function(g)heart_data$Class[cl2$cluster ==g])
classDivision
g1 = c()
g1 = classDivision[[1]]
length(g1)
g2 = c()
g2 = classDivision[[2]]
length(g2)
g3 = c()
g3 = classDivision[[3]]
length(g3)
g4 = c()
g4 = classDivision[[4]]
g4
length(g4)
g5 = c()
g5 = classDivision[[5]]
length(g5)
count1s = 0
count2s = 0
count1s = 0
count2s = 0
i =0
for (i in 1:length(g)){
if(g1[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
i =0
for (i in 1:length(g1)){
if(g1[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
count1s = 0
count2s = 0
i =0
for (i in 1:length(g2)){
if(g2[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
count1s = 0
count2s = 0
i =0
for (i in 1:length(g3)){
if(g3[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
count1s = 0
count2s = 0
i =0
for (i in 1:length(g3)){
if(g3[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
count1s = 0
count2s = 0
i =0
for (i in 1:length(g4)){
if(g4[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
count1s = 0
count2s = 0
i =0
for (i in 1:length(g5)){
if(g5[i] ==1)
{
count1s = count1s + 1
}
else
{
count2s = count2s +1
}
}
count2s/ (count1s + count2s)
print(cl2)
cl2$mean
cl2$means
round(apply(KMD, 2, mean), 2)
print(cl2)
classDivision = sapply(unique(cl2$cluster), function(g)heart_data$Class[cl2$cluster ==g])
classDivision
heart_data$Class
print(cl2)
round(apply(KMD, 2, mean), 2)
print(cl2)
################################################################################################################################################
#MLA Assignment
################################################################################################################################################
#Read csv file
heart_data<-read.csv("C:/Users/carty/Documents/Third Year MSISS/MLA Assignment/heart_data.csv")
heart_data
summary(heart_data)
dim(heart_data)
library(ellipse)
library(MASS)
library(dplyr)
class1data<-heart_data %>% filter(Class==1)
class2data<-heart_data %>% filter(Class==2)
dim(class1data)
dim(class2data)
plot(heart_data[,c(1,3,4,6)], col = as.factor(heart_data[,10]))
#Before we use LDA, we need to split the data into training, and test sets. (Why don't we need a validation set?)
#For this example we will try an 80:20 split.
strain <- heart_data[c(1:150), ]
stest <- heart_data[c(151:270),]
train <- rbind(class1data[1:75,],class2data[1:60,])
test <- rbind(class1data[76:150,],class2data[61:120,])
#Before we use LDA, we need to split the data into training, and test sets. (Why don't we need a validation set?)
#For this example we will try an 80:20 split.
strain <- heart_data[c(1:150), ]
stest <- heart_data[c(151:270),]
train <- rbind(class1data[1:75,],class2data[1:60,])
test <- rbind(class1data[76:150,],class2data[61:120,])
#We can then train our classifier:
lsol <- lda(train[, 1:9], grouping = train[,10])
#If you enter the following, you will be returned with a list of summary information concerning the computation:
lsol$prior
lsol$means
#To estimate the covariance matrices for both subsets of salmon data, enter the following:
n_class_1 <- length(class1data)
n_class_2 <- length(class2data)
single_cov_num <- ((n_class_1 - 1) * cov (class1data) + (n_class_2 - 1) * cov(class2data) )
single_cov <- single_cov_num / ( length(train[, 10]) - 2)
single_cov
#As well as providing information about prior probalilities and group means, calling lsol directly provides
#information regarding the coefficients of linear discriminants (use lsol$scaling to call this directly):
lsol
lsol$scaling
#As well as providing information about prior probalilities and group means, calling lsol directly provides
#information regarding the coefficients of linear discriminants (use lsol$scaling to call this directly):
lsol
lsol$scaling
#To automatically predict the test data set enter:
predict(lsol, test[, c(1,3,4,6,8,9)])
#Cross-Validation
#an alternative technique for measuring the performance of the model is to perform cross-validation
lsol_cv <- lda(heart_data[,c(1,3,4,6)], grouping = heart_data[, 10], CV = TRUE)
#Before we use LDA, we need to split the data into training, and test sets. (Why don't we need a validation set?)
#For this example we will try an 80:20 split.
strain <- heart_data[c(1:150), ]
stest <- heart_data[c(151:270),]
train <- rbind(class1data[1:75,],class2data[1:60,])
test <- rbind(class1data[76:150,],class2data[61:120,])
#We can then train our classifier:
lsol <- lda(strain[, 1:9], grouping = strain[,10])
#If you enter the following, you will be returned with a list of summary information concerning the computation:
lsol$prior
lsol$means
#Before we use LDA, we need to split the data into training, and test sets. (Why don't we need a validation set?)
#For this example we will try an 80:20 split.
strain_index <- c(1:150)
stest_index <- c(151:270)
strain <- heart_data[strain_index , ]
stest <- heart_data[stest_index,]
#We can then train our classifier:
lsol <- lda(strain[, 1:9], grouping = strain[,10])
#If you enter the following, you will be returned with a list of summary information concerning the computation:
lsol$prior
lsol$means
class_agree <- table(lsol, heart_data[strain_index,10])
class_agree
#If you enter the following, you will be returned with a list of summary information concerning the computation:
lsol
require(fma)
tsdisplay(beer)
?Arima
Arima(beer, order=c(0,0,0),seasonal = c(1,0,0))
plot(Arima(beer, order=c(0,0,0),seasonal = c(1,0,0)))
?bricksq
tsdisplay(bricksq)
Arima(bricksq, order=c(0,0,0), seasonal = c(1,0,0))
Arima(bricksq, order=c(0,1,0), seasonal = c(1,0,0))
Arima(bricksq, order=c(0,1,0), seasonal = c(1,0,0))
Arima(bricksq, order=c(0,1,0), seasonal = c(1,0,0),include.drift=TRUE)
tsdisplay(Arima(bricksq, order=c(0,1,0), seasonal = c(1,0,0),include.drift=TRUE)$residuals0
tsdisplay(Arima(bricksq, order=c(0,1,0), seasonal = c(1,0,0),include.drift=TRUE)$residuals)
tsdisplay(Arima(bricksq,order=c(0,0,0),seasonal=c(0,0,0),include.drift = TRUE)$residuals)
tsdisplay(Arima(bricksq,order=c(0,0,0),seasonal=c(0,0,0),include.drift = FALSE)$residuals)
#Fitting an ARIMA Model
tsdisplay(Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = TRUE)$residuals)
tsdisplay(Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = FALSE)$residuals)
Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = TRUE)$residuals
Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = TRUE)
Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = FALSE)
Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = FALSE)
#Fitting an ARIMA Model: Minimize AIC
tsdisplay(Arima(bricksq,order=c(0,0,0),seasonal=c(0,0,0),include.drift = FALSE)$residuals)
Arima(bricksq,order=c(0,0,0),seasonal=c(0,0,0),include.drift = FALSE)
Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = FALSE)
Arima(bricksq,order=c(0,1,0),seasonal=c(0,0,0),include.drift = TRUE)
tsdisplay(Arima(bricksq,order=c(0,1,0),seasonal=c(0,1,0),include.drift = FALSE)$residuals)
Arima(bricksq,order=c(0,1,0),seasonal=c(0,1,0),include.drift = FALSE)
classDivision
classDivision
q()
